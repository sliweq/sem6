{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ac5d8c",
   "metadata": {},
   "source": [
    "# Lista 4\n",
    "\n",
    "## 1. Wstęp\n",
    "Na liście 4 celem jest analiza oraz klasyfikacja danych parametrów życiowych płodu z wykorzstanie algorytmów uczenia maszynowego. Zastosowane zostały 4 różne algorytmy. Naive Bayes, Decision Tree, Random Forest, SVR. Porównane zostały skuteczność poszczególnych metod pod kątem miar jakości predykcji. Dodatkowo przeanalizowano wpływ wybranych parametrów na działanie modeli oraz zastosowane techniki przetwarzania danych.\n",
    "\n",
    "\n",
    "## 2. Implementacja\n",
    "Generowanie raportu zawierającego informacje o zbiorze wejsciowym, który jest częściowo niepełny oraz zanieczyszczony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07594ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "file = Path(\"csv_data_analyse.html\")\n",
    "if not file.exists():\n",
    "    df = pd.read_csv(\"CTG.csv\", sep=\";\", decimal=\",\")\n",
    "    profiler = ProfileReport(df, title=\"Data Profiler\")\n",
    "    profiler.to_file(output_file=\"csv_data_analyse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ed7e5",
   "metadata": {},
   "source": [
    "Czyszczenie danych wejściowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "958b2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CTG.csv\", sep=\";\", decimal=\",\")\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8b7ba",
   "metadata": {},
   "source": [
    "Podział na zbiór testowy i uczący. Stosunek zbiorów to 3:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f2bd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=[\"CLASS\"])\n",
    "y = df[\"CLASS\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f86433",
   "metadata": {},
   "source": [
    "### Normalizacja oraz Standaryzacja\n",
    "StandardScaler skaluje globalnie dane tak, aby ich średnia wynosiła 0, a odchylenie standardowe wynosiło 1. Natomiast Normalizer przekształca dane tak aby kazdy wektor cech miał długość równą 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f18011b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# data normalization\n",
    "standardScaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer_x_train = normalizer.fit_transform(X_train)\n",
    "normalizer_x_test = normalizer.transform(X_test)\n",
    "\n",
    "scalled_x_train = standardScaler.fit_transform(X_train)\n",
    "scalled_x_test = standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc49773",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes to statystyczna technika klasyfikacji oparta na twierdzeniu Bayesa. Jest to jeden z najprostszych algorytmów uczenia nadzorowanego, który cechuje się szybkością, dokładnością i niezawodnością, zwłaszcza przy dużych zbiorach danych.\n",
    "\n",
    "Algorytm zakłada, że wpływ każdej cechy na przynależność do danej klasy jest niezależny od pozostałych cech nawet jeśli w rzeczywistości są one ze sobą powiązane. To uproszczenie pozwala znacząco zredukować złożoność obliczeniową, dlatego algorytm określa się jako „naiwny”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9e4892c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "Standard 1e-09 accuracy: 0.881578947368421\n",
      "Standard 1e-09 accuracy: 1.0\n",
      "Standard 1e-12 accuracy: 0.8609022556390977\n",
      "Standard 1e-12 accuracy: 1.0\n",
      "Standard 1e-12 accuracy: 0.8740601503759399\n",
      "Standard 1e-12 accuracy: 1.0\n",
      "Standard 0.001 accuracy: 0.37969924812030076\n",
      "Standard 0.001 accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def calculate(clasifier_name, classifer, x_test, y_test, x_train, y_train, file):\n",
    "    classifer.fit(x_train,y_train)\n",
    "    pred = classifer.predict(x_test)\n",
    "    with open(file,\"w\") as f:\n",
    "        f.write(f\"Accuracy {accuracy_score(y_test, pred)}\")\n",
    "        f.write(classification_report(y_test, pred, zero_division=0))\n",
    "\n",
    "    print(f\"{clasifier_name} accuracy:\", accuracy_score(y_test, pred))\n",
    "\n",
    "gaussians = [\n",
    "    (\"Standard 1e-09\",GaussianNB(),\"Gaussian/09\"),\n",
    "    (\"Standard 1e-12\",GaussianNB(var_smoothing=1e-12), \"Gaussian/12\"),\n",
    "    (\"Standard 1e-12\",GaussianNB(var_smoothing=1e-6), \"Gaussian/06\"),\n",
    "    (\"Standard 0.001\",GaussianNB(var_smoothing=0.001), \"Gaussian/1\")\n",
    "]\n",
    "print(\"GaussianNB\")\n",
    "for name, gaussian, file in gaussians:\n",
    "    calculate(name,gaussian,normalizer_x_test,y_test,normalizer_x_train,y_train,file+\"standard\")\n",
    "    calculate(name,gaussian,scalled_x_test,y_test,scalled_x_train,y_train,file+\"scalled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee83a0",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Drzewa decyzyjne to struktura przypominająca diagram przepływu, składają się z węzłów reprezentujących\n",
    "decyzje, gałęzi reprezentujących wynik tych decyzji oraz węzłów liści reprezentujących ostateczne wyniki.\n",
    "Ważnymi elemntami każdego drzewa jest korzeń (root) reprezentujący zbiór danych i początkową decyzję do\n",
    "podjęcia. Kolejnym ważnym elementem jest wewnętrzny węzeł (internal node) reprezentują decyzje lub testy\n",
    "na atrybutach. Gałąź (branch) jest wynikiem decyzji lub testu i prowadzadzi do innego węzła. Każdy node\n",
    "może mieć wiele gałęzi. Węzęły liści (leaf nodes) czyli końcowewęzły zawierające wynik.\n",
    "\n",
    "Działanie drzewa decyzyjnego:\n",
    "\n",
    "1. Drzewo wybiera najlepszy atrybut do podziału danych. Wybierany jest on przy użyciu odpowiednich miar\n",
    "2. Podział zbioru danych na podstawie wybranego atrybutu\n",
    "3. Proces jest powtarzany rekurencyjnie dla każdego podzbioru, tworząc nowy węzeł wewnętrzny lub węzeł liścia, aż do spełnienia kryterium zatrzymania\n",
    "    1. Liczba próbek w węźle jest mniejsza niż określony limit\n",
    "    2. Głębokość węzła jest większa niż określony limit\n",
    "    3. Czystość węzła jest większa niż pre-definiowany limit\n",
    "    4. Wartości predyktorów dla wszystkich rekordów są identyczne, co uniemożliwia wygenerowanie reguły do ich podziału\n",
    "\n",
    "Metryki uzywane do drzewa podziału drzewa decyzyjnego:\n",
    "1. Entropy - Mierzy ilość niepewności lub nieczystości w zbiorze danych: $$ 1 − \\displaystyle\\sum_{i=1}^n (p_i)^2 $$\n",
    "2. Gini - Służy do przewidywania prawdopodobieństwa nieprawidłowej klasyfikacji losowo wybranego przykładu przez konkretny węzeł:  $$ − \\displaystyle\\sum_{i=1}^n p_i log_2(p_i) $$\n",
    "3. Information Gain - Mierzy redukcję entropii lub nieczystości Gini po podziale zbioru danych na podstawie atrybutu : $$ Entropia_{parent} −  \\displaystyle\\sum_{i=1}^n ( \\frac{|D_i|}{|C|}  \\times  Entropia(D_i)) $$ \n",
    ".\n",
    "\n",
    "$$ (p_i) - prawdopodobieństwo przypisania przypadku do danej klasy, |D_i|  - podzbiór |D|  po podziale na podstawie atrybutu $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "90fd80b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Default accuracy: 1.0\n",
      "Default accuracy: 0.9981203007518797\n",
      "Entropy accuracy: 1.0\n",
      "Entropy accuracy: 0.9943609022556391\n",
      "Log_loss accuracy: 1.0\n",
      "Log_loss accuracy: 0.9943609022556391\n",
      "Default d=5 accuracy: 0.881578947368421\n",
      "Default d=5 accuracy: 0.881578947368421\n",
      "Default d=10 accuracy: 1.0\n",
      "Default d=10 accuracy: 0.9962406015037594\n",
      "Default d=20 accuracy: 1.0\n",
      "Default d=20 accuracy: 0.9981203007518797\n",
      "Default min accuracy: 1.0\n",
      "Default min accuracy: 0.9981203007518797\n",
      "Default alpha accuracy: 1.0\n",
      "Default alpha accuracy: 0.9962406015037594\n",
      "Default all accuracy: 0.881578947368421\n",
      "Default all accuracy: 0.881578947368421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "trees = [\n",
    "    (\"Default\", DecisionTreeClassifier(), \"Tree/normal\"),\n",
    "    (\"Entropy\", DecisionTreeClassifier(criterion=\"entropy\"),\"Tree/entropy\"),\n",
    "    (\"Log_loss\", DecisionTreeClassifier(criterion=\"log_loss\"),\"Tree/log\"), \n",
    "    # łagodzenie\n",
    "    (\"Default d=5\", DecisionTreeClassifier(max_depth=5), \"Tree/normal5\"),\n",
    "    (\"Default d=10\", DecisionTreeClassifier(max_depth=10), \"Tree/normal10\"),\n",
    "    (\"Default d=20\", DecisionTreeClassifier(max_depth=20), \"Tree/normal20\"),\n",
    "    (\"Default min\", DecisionTreeClassifier(max_depth=10, min_samples_leaf=5, min_samples_split=5), \"Tree/min\"),\n",
    "    (\"Default alpha\" , DecisionTreeClassifier(max_depth=10, ccp_alpha=0.01), \"Tree/alpha\"),\n",
    "    (\"Default all\", DecisionTreeClassifier(max_depth=10, min_samples_leaf=5, min_samples_split=5, ccp_alpha=0.05), \"Tree/all\"),\n",
    "]\n",
    "\n",
    "print(\"DecisionTreeClassifier\")\n",
    "for name, tree, file in trees:\n",
    "    calculate(name,tree,normalizer_x_test,y_test,normalizer_x_train,y_train,file+\"standard\")\n",
    "    calculate(name,tree,scalled_x_test,y_test,scalled_x_train,y_train,file+\"scalled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa35fa7",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Random Forest to metoda uczenia się zespołowego, która łączy predykcje z wielu drzew decyzyjnych w celu uzyskania dokładniejszych i stabilnych wyników. \n",
    "Random Forest używa się zarówno do Regresji jak i Klasyfikacji. Random Forest składa sie z wielu Drzew Decyzyjnych. Każde drzewo ma tak zwane bootstrap samples.\n",
    "Bootstrap samples to podzbiór oryginalnego zbioru. Podzbiór ten to losowo wybierane dane. Ważne jest żeby podzbiór ten finalnie zawierał rówież powtórzenia. Oczekuje się 63,2% unikalnych danych.\n",
    "Dla klażdego subsetu wybiera sie również podzbiór niepowtarzalnych cech. Każde drzewo jest trenowane. Wynik w przypadku regresji jest średnią, natomiast w przypadku klasyfikacji wybierana \n",
    "jest odpowiedź występująca najczęściej. Cały proces nazywany jest baggingiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "599ec554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "Default accuracy: 1.0\n",
      "Default accuracy: 1.0\n",
      "entropy accuracy: 1.0\n",
      "entropy accuracy: 1.0\n",
      "log accuracy: 1.0\n",
      "log accuracy: 1.0\n",
      "Default depth n = 5 accuracy: 1.0\n",
      "Default depth n = 5 accuracy: 1.0\n",
      "Default depth n = 15 accuracy: 1.0\n",
      "Default depth n = 15 accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forests = [\n",
    "    (\"Default\", RandomForestClassifier(criterion=\"gini\"), \"Random/normal\"),\n",
    "    (\"entropy\", RandomForestClassifier(criterion=\"entropy\"), \"Random/entropy\"),\n",
    "    (\"log\", RandomForestClassifier(criterion=\"log_loss\"),\"Random/log\"),\n",
    "    (\"Default depth n = 5\", RandomForestClassifier(criterion=\"gini\", max_depth=10), \"Random/normal5\"),\n",
    "    (\"Default depth n = 15\", RandomForestClassifier(criterion=\"gini\", max_depth=15), \"Random/normal5\"),\n",
    "]\n",
    "\n",
    "print(\"RandomForestClassifier\")\n",
    "for name, forest, file in forests:\n",
    "    calculate(name,forest,normalizer_x_test,y_test,normalizer_x_train,y_train,file+\"standard\")\n",
    "    calculate(name,forest,scalled_x_test,y_test,scalled_x_train,y_train,file+\"scalled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d92ac",
   "metadata": {},
   "source": [
    "### SVR\n",
    "\n",
    "Support Vector Regression (SVR) jest rozszerzeniem Support Vector Machines. Główną ideą SVR jest wykorzystanie hiperpłaszczyzny w przestrzeni cech danych, która dzieli dane na dwie klasy, reprezentujące różnice między wartościami wyjściowymi. SVR jest algorytmem opartym na odległości, dlatego skalowanie jest ważnym etapem przetwarzania wstępnego, który może poprawić dokładność i stabilność modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854294ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "liner\n",
      "R2 score: -0.0362, RMSE: 3.1837\n",
      "liner\n",
      "R2 score: 0.9990, RMSE: 0.0999\n",
      "poly\n",
      "R2 score: 0.0284, RMSE: 3.0828\n",
      "poly\n",
      "R2 score: 0.9619, RMSE: 0.6101\n",
      "rbf\n",
      "R2 score: -0.0100, RMSE: 3.1432\n",
      "rbf\n",
      "R2 score: 0.9886, RMSE: 0.3345\n",
      "sigmoid\n",
      "R2 score: -0.0429, RMSE: 3.1940\n",
      "sigmoid\n",
      "R2 score: -1.2070, RMSE: 4.6463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "def calculate_regression(model_name, model, x_test, y_test, x_train, y_train, file):\n",
    "    print(model_name)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred = model.predict(x_test)\n",
    "\n",
    "    with open(file, \"w\") as f:\n",
    "        f.write(f\"R2 score: {r2_score(y_test, pred)}\\n\")\n",
    "        f.write(f\"RMSE: {root_mean_squared_error(y_test, pred)}\\n\")\n",
    "\n",
    "    print(f\"R2 score: {r2_score(y_test, pred):.4f}, RMSE: {root_mean_squared_error(y_test, pred):.4f}\")\n",
    "\n",
    "\n",
    "svrs = [\n",
    "    (\"liner\", SVR(kernel=\"linear\"), \"SVR/linear\"),\n",
    "    (\"poly\", SVR(kernel=\"poly\"), \"SVR/poly\"),\n",
    "    (\"rbf\", SVR(kernel=\"rbf\"), \"SVR/rbf\"),\n",
    "    (\"sigmoid\", SVR(kernel=\"sigmoid\"), \"SVR/sigmoid\"),\n",
    "]\n",
    "\n",
    "print(\"SVR\")\n",
    "for name, svr, file in svrs:\n",
    "    calculate_regression(name,svr,normalizer_x_test,y_test,normalizer_x_train,y_train,file+\"standard\")\n",
    "    calculate_regression(name,svr,scalled_x_test,y_test,scalled_x_train,y_train,file+\"scalled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3be85",
   "metadata": {},
   "source": [
    "### Analiza danych\n",
    "Wprzypadku Bayes'a dokładność algorytmu wychodziła znacznie lepiej w przypadku skalowania. W przypadku normalizacji algorytm miał nico mniejsza dokładność.\n",
    "W przypadku drzew decyzyjnych to normalizacja danych powdowała że dokładność była lepsza. W przypadku Random forest'a dokładnośc wychodziła na podobnym poziomie w obu danych. \n",
    "W przypadku SVR wybór odpowiedniego kernela oraz przetworzenie danych wejścowych. W przypadku normalizacji wszystkie kerenle osiągneły bardzo słaby wyniki zarówno w R^2 jak i RMSE. Natomiast standaryzacja jest znacznie lepszym sposobem przetowrzenia danych. W Przyapdku linear kernel dane dopasowały sie niemal idealnie. W przypadku rbf oraz poly dopasowanie również było bardzo dobre oraz dobre. Natomiast najgorszy miał kernel sigmoid. \n",
    "\n",
    "W przypadku plików wynikowych dla metody Bayes wyniki były na popziomie 50% dla normalizacji. Prezycja z jaką algorytm działał była mierna. Podobnie jak czułość która również była mierna. Najlepszym sposobem oceny jest f-1 score który jest średnią harmoniczną precision oraz recall. Najlepsze wyniki dla normalizacji były najlepsze dla var_smothing rzędu 1e-07/1e-08. Dla zeskalowanych danych dla każdego smothingu dokładność była rzędu 1. \n",
    "\n",
    "W przypadku drzew decyzyjnych dokładnośc była rzędu 1 w przypadku danych które zostały znormalizowane. Dla danych zeskalowanych dokładność była rzędu 0.99. W przypadku kolejnych drzew podczas testów dobór takich parametrów jak max_depth, min_samples_split (minimalna liczba próbek wymagana aby węzeł został podzielony), min_samples_leaf (minimalna liczba próbek wymagana w końcowym węźle), ccp_alpham, pozwalały na manipulowanie trenowaniem, czym wpływały na wynik.\n",
    "Głowny wpływ na prztrenowania ma dobór parametrów. Parametr cpp_alpha pozwoił w znacznym stopniu manipulować dokładnością. Cost-complexity pruning alpha funkcja ta pozwala zmniejszać koszt oraz upraszcza drzewo, przycinając je. Zapobiega to w głównej mierze przed overfittingiem.    \n",
    "\n",
    "W przypadku Random Forrest Cieżko stwirdzić jakieś zależności ze względu na dokładność rzędu 1. Ze względu na losowośc tego algorytmu czasem zdarzało się ze dokładność modeli w których kryterium była entrpia, lekko spadał do nawet dokładnośći rzędu 0,95."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30e0f5",
   "metadata": {},
   "source": [
    "## 3. Podsumowanie\n",
    "Ze wzgędu na poprzednie doświadczenie w jedym projekcie z SVR oraz Random Forrest ćwiczenie było dużo łatwiejsze w realizacji. \n",
    "W przypadku każdeg oalgorymtu należy uważnie dobierać parametry oraz dane. Sama zabawa z prostymi algorytmami uczenia maszynowego wymaga nawet minimalnego przygotowania oraz wiedzy aby umyślnie dobierać parametry. Jak zostało zaprezentowane powyżej nawet dobne zmiany moga prowadzić do zmian. Również zbiór danych oraz odpowiednie przygotowanie go może wpłynąć na wynik oraz dokładność modelu. Kolejnym ważnym aspektem na który należy zwracać uwagę jest również przetrenowanie modeli, co również wiąże się z dobieraniem odpowienich parametrów algorytmów.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd1143d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
